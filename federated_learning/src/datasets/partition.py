"""Represents a module, which implements different data partitioning strategies for the federated learning setting."""

from typing import Any

import numpy
from federated_learning.src.datasets.dataset import Dataset


class DataPartitioner:
    """Represents a class for data partitioning."""

    def __init__(
        self,
        train_dataset_instance: Dataset,
        number_of_clients: int,
        beta: float
    ) -> None:
        """Initializes a new DataPartitioner instance.

        Args:
            train_dataset_instance (Dataset): The instance of the  dataset that is to be partitioned.
            number_of_clients (int): Number of clients to partition the dataset.
            beta (float): The parameter of the dirichlet distribution which controls the heterogeneity of the clients' local data.
        """

        # Stores the arguments
        self.train_dataset = train_dataset_instance.training_data
        self.number_of_clients = number_of_clients
        self.beta = beta
        self.number_of_samples = len(self.train_dataset)
        self.labels = train_dataset_instance.get_labels()
        self.number_of_labels = train_dataset_instance.number_of_classes

    def partition_data_homogeneous(self) -> dict[int, numpy.ndarray[Any, Any]]:
        """
        Distributes the specified data in such a way, that the data distribution for each client is i.i.d.

        Returns:
            dict[int, numpy.ndarray[Any, Any]]: Returns a dictionary that contains for each client index the assigned training indices.
        """

        number_of_samples_per_client_for_training = int(self.number_of_samples/self.number_of_clients)
        available_training_data_indices = numpy.arange(self.number_of_samples, dtype=numpy.int64).tolist()
        client_index_to_train_indices = {i: numpy.array([], dtype=numpy.int64) for i in range(self.number_of_clients)}

        # Distributes  training data indices for each client in an iid fashion
        for client_index in range(self.number_of_clients):
            selected_training_data_indices = set(numpy.random.choice(
                available_training_data_indices,
                number_of_samples_per_client_for_training,
                replace=False)
            )
            client_index_to_train_indices[client_index] = numpy.array(selected_training_data_indices)
            available_training_data_indices = list(set(available_training_data_indices) - selected_training_data_indices)

        return client_index_to_train_indices

    def partition_data_based_on_dirichlet_generated_label_imbalances(self) -> dict[int, numpy.ndarray[Any, Any]]:
        """
        Partitions the clients in such a way that each client has an imbalance in the label distribution generated by the dirichlet distribution

        Returns:
            dict[int, numpy.ndarray[Any, Any]]: Returns a dictionary that contains for each client index the assigned training indices.
        """
        # Ensures that each client will have at least enough samples to have at least one sample for every label
        min_size = 0
        min_require_size = self.number_of_labels
        unique_labels = numpy.arange(self.number_of_labels)
        client_index_to_train_indices = {i: numpy.array([], dtype=numpy.int64) for i in range(self.number_of_clients)}

        available_samples_for_distribution = self.number_of_samples


        # Distributes the data among the clients
        while min_size < min_require_size:
            client_index_train_indices_map: list[list[int]]
            client_index_train_indices_map = [[] for _ in range(self.number_of_clients)]
            # Retrieves the indices for each label and assigns them to clients based on dirichlet distribution
            for label in unique_labels:
                label_indices_for_one_label = numpy.where(self.labels == label)[0]
                numpy.random.shuffle(label_indices_for_one_label)
                proportions = numpy.random.dirichlet(numpy.repeat(self.beta, self.number_of_clients))
                # Masks the generated proportions to ensure that none of the clients exceed the available
                proportions = numpy.array([proportion * (len(client_train_indices) < available_samples_for_distribution / self.number_of_clients)
                                          for proportion, client_train_indices in zip(proportions, client_index_train_indices_map)])
                proportions = proportions / proportions.sum()
                # Generates the splitting index  to split the label indices for a given label
                split_indices = (numpy.cumsum(proportions) * len(label_indices_for_one_label)).astype(int)[:-1]
                new_client_index_train_indices_map = numpy.split(label_indices_for_one_label, split_indices)
                client_index_train_indices_map = [client_indices_old + client_indices_new.tolist() for client_indices_old, client_indices_new
                                                  in zip(client_index_train_indices_map, new_client_index_train_indices_map)]

                # Checks if all clients have the minimum required sample size
                min_size = min([len(client_train_indices) for client_train_indices in client_index_train_indices_map])
        client_index_to_train_indices = {i: numpy.array(indices)
                                         for i, indices in enumerate(client_index_train_indices_map) if indices}
        new_client_index_mapping = {old_index: new_index for new_index, old_index in enumerate(client_index_to_train_indices.keys())}
        client_index_to_train_indices = {new_client_index_mapping[old_index]: indices
                                         for old_index, indices in client_index_to_train_indices.items()}

        return client_index_to_train_indices

    def partition_data_with_varying_sample_size(self) -> dict[int, numpy.ndarray[Any, Any]]:
        """ Splits data in an i.i.d fashion with varying sample size among the clients.
        Returns:
            dict[int, numpy.ndarray[Any, Any]]: Returns a dictionary that contains for each client index the assigned training indices.
        """
         # Shuffle indices of the samples
        training_indices = numpy.random.permutation(self.number_of_samples)

        # Initial Dirichlet distribution to determine proportions
        raw_proportions = numpy.random.dirichlet(numpy.repeat(self.beta, self.number_of_clients))

        # Adjust proportions to ensure a minimum number of samples for each client
        min_samples_per_client = 5
        min_proportion = min_samples_per_client / len(training_indices)

        # Scale up raw proportions while ensuring the minimum is met
        adjusted_proportions = raw_proportions * (1 - self.number_of_clients * min_proportion) + min_proportion

        # Split indices based on adjusted proportions
        split_indices = (numpy.cumsum(adjusted_proportions) * len(training_indices)).astype(int)[:-1]
        client_training_indices_split = numpy.split(training_indices, split_indices)
        client_index_training_indices_map = {i: client_training_indices_split[i] for i in range(self.number_of_clients)}

        return client_index_training_indices_map

    def partition_data_with_fixed_subset_of_labels(self, number_of_assigned_labels: int) -> dict[int, numpy.ndarray[Any, Any]]:
        """Splits the data in such a way, that each client only has a specified number of distinct labels.

        Args:
            number_of_assigned_labels (int): The number of different labels for each client.
        Returns:
            dict[int, numpy.ndarray[Any, Any]]: Returns a dictionary that contains for each client index the assigned training indices.
        """
        unique_labels = numpy.arange(self.number_of_labels)

        # Keeps track how often a specific label is distributed to a client
        label_counts_map = [0 for _ in range(self.number_of_labels)]

        # Stores the labels for each client it has been assigned
        client_index_unique_labels_map = []

        # Assings each client distinct labels, where the number of assinged labels depends on labels_per_client
        for client_index in range(self.number_of_clients):
            current_label = []
            unique_label_count = 0
            while unique_label_count < number_of_assigned_labels:
                random_label_index = numpy.random.randint(0, self.number_of_labels)
                if random_label_index not in current_label:
                    unique_label_count += 1
                    current_label.append(random_label_index)
                    label_counts_map[random_label_index] += 1
            client_index_unique_labels_map.append(current_label)

        client_index_to_train_indices = {i: numpy.array([], dtype=numpy.int64) for i in range(self.number_of_clients)}
        for label in unique_labels:
            label_indices = numpy.where(self.labels == label)[0]
            numpy.random.shuffle(label_indices)
            # Splits number of times specified in label_counts that is the number of clients that have been assigned this class.
            split = numpy.array_split(label_indices, label_counts_map[label])
            split_index = 0
            for client_index in range(self.number_of_clients):
                if label in client_index_unique_labels_map[client_index]:
                    client_index_to_train_indices[client_index] = numpy.append(
                        client_index_to_train_indices[client_index],
                        split[split_index]
                    )
                    split_index += 1
        # Sanity check if all data points were distributed
        all_data = 0
        for clients_indices in client_index_to_train_indices.values():
            all_data += len(clients_indices)
        assert all_data == self.number_of_samples

        return client_index_to_train_indices

    def partition_real_world(self) -> dict[int, numpy.ndarray[Any, Any]]:
        """
        Distributes the specified data in such a way, that the data distribution for each client is i.i.d.

        Returns:
            dict[int, numpy.ndarray[Any, Any]]: Returns a dictionary that contains for each client index the assigned training indices.
        """
        # Extract user indices (now strings) and determine unique users
        user_for_training = numpy.array(self.train_dataset.users_index)
        unique_users, _ = numpy.unique(user_for_training, return_counts=True)

        # Initialize the user index mapping to accumulate data indices for each user
        user_indices = {user: [] for user in unique_users}
        for user_index, user in enumerate(user_for_training):
            user_indices[user].append(user_index)

        # Map each client to its corresponding user's data indices
        client_index_to_train_indices = {i: numpy.array([], dtype=numpy.int64) for i in range(self.number_of_clients)}
        for i, user in enumerate(unique_users):
            client_index_to_train_indices[i] = numpy.array(user_indices[user])

        return client_index_to_train_indices






